"""
AI å±¤æ•´åˆ API
æä¾› AI/NLP æœå‹™èª¿ç”¨çš„å°ˆç”¨æ¥å£
"""
from fastapi import APIRouter, HTTPException, Query, BackgroundTasks
from typing import List, Optional
from ..models.schemas import AIAnalysisRequest, AIAnalysisResult, Cluster
from ..services.question_service import question_service
from ..services.ai_service import ai_service


router = APIRouter(prefix="/ai", tags=["ai-integration"])


@router.get("/questions/pending", response_model=dict, summary="å–å¾—å¾… AI åˆ†æçš„æå•")
async def get_pending_questions_for_ai(
    course_id: str = Query(..., description="èª²ç¨‹ID"),
    limit: int = Query(100, ge=1, le=500, description="é™åˆ¶ç­†æ•¸")
):
    """
    å–å¾—å¾… AI åˆ†æçš„æå•åˆ—è¡¨
    
    **æ­¤ API åƒ…è¿”å›å»è­˜åˆ¥åŒ–å¾Œçš„è³‡æ–™**ï¼š
    - pseudonym (å»è­˜åˆ¥åŒ–ä»£è™Ÿ)
    - question_text (æå•å…§å®¹)
    - ä¸åŒ…å«ä»»ä½•å¯è­˜åˆ¥å€‹äººèº«ä»½çš„è³‡è¨Š
    
    **ç”± AI/NLP æœå‹™å®šæœŸèª¿ç”¨**
    """
    questions = await question_service.get_pending_questions_for_ai(
        course_id, limit
    )
    
    return {
        "success": True,
        "data": questions,
        "total": len(questions)
    }


@router.post("/analysis/batch", response_model=dict, summary="æ‰¹æ¬¡å¯«å…¥ AI åˆ†æçµæœ")
async def batch_update_ai_analysis(
    results: List[AIAnalysisResult]
):
    """
    æ‰¹æ¬¡å¯«å…¥ AI åˆ†æçµæœ
    
    **æ­¤ API ç”± AI/NLP æœå‹™èª¿ç”¨**
    
    æ¥æ”¶ AI åˆ†æçµæœä¸¦æ›´æ–°è‡³è³‡æ–™åº«ï¼š
    - cluster_id: AI èšé¡ID
    - difficulty_score: é›£åº¦åˆ†æ•¸ (0-1)
    - keywords: é—œéµå­—åˆ—è¡¨
    """
    success_count = 0
    failed_count = 0
    errors = []
    
    for result in results:
        try:
            question = await question_service.update_ai_analysis(
                result.question_id,
                result
            )
            if question:
                success_count += 1
            else:
                failed_count += 1
                errors.append({
                    "question_id": result.question_id,
                    "error": "æ‰¾ä¸åˆ°æ­¤æå•"
                })
        except Exception as e:
            failed_count += 1
            errors.append({
                "question_id": result.question_id,
                "error": str(e)
            })
    
    return {
        "success": True,
        "message": f"æˆåŠŸæ›´æ–° {success_count} ç­†ï¼Œå¤±æ•— {failed_count} ç­†",
        "success_count": success_count,
        "failed_count": failed_count,
        "errors": errors if errors else None
    }


@router.post("/analysis/single", response_model=dict, summary="å–®ç­†å¯«å…¥ AI åˆ†æçµæœ")
async def single_update_ai_analysis(
    result: AIAnalysisResult
):
    """
    å–®ç­†å¯«å…¥ AI åˆ†æçµæœ
    
    **æ­¤ API ç”± AI/NLP æœå‹™èª¿ç”¨**
    """
    question = await question_service.update_ai_analysis(
        result.question_id,
        result
    )
    
    if not question:
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æ­¤æå•")
    
    return {
        "success": True,
        "message": "AI åˆ†æçµæœæ›´æ–°æˆåŠŸ",
        "data": question
    }

@router.post("/questions/{question_id}/draft", summary="ç”Ÿæˆ/é‡å¯«å•é¡Œçš„å›è¦†è‰ç¨¿")
async def generate_response_draft(
    question_id: str,
    background_tasks: BackgroundTasks
):
    """
    è§¸ç™¼ AI ç‚ºç‰¹å®šå•é¡Œç”Ÿæˆå›è¦†è‰ç¨¿
    """
    # 1. å–å¾—å•é¡Œè³‡æ–™
    question = await question_service.get_question(question_id)
    if not question:
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æ­¤æå•")

    # 2. å®šç¾©èƒŒæ™¯ä»»å‹™å‡½æ•¸
    async def _generate_and_save_draft(qid: str, text: str):
        try:
            # å‘¼å« AI ç”Ÿæˆè‰ç¨¿
            draft = await ai_service.generate_response_draft(text)
            
            # å‘¼å« AI ç”Ÿæˆæ‘˜è¦ (é †ä¾¿åš)
            analysis = await ai_service.analyze_question(text)
            summary = analysis.get("summary", "")
            
            # æ§‹é€ æ›´æ–°ç‰©ä»¶ (åˆ©ç”¨ç¾æœ‰çš„ update_ai_analysis ä»‹é¢)
            # æ³¨æ„ï¼šé€™è£¡å‡è¨­æ‚¨å·²ç¶“åœ¨ schemas.py çš„ AIAnalysisResult åŠ å…¥äº† response_draft æ¬„ä½
            result = AIAnalysisResult(
                question_id=qid,
                difficulty_score=question.get("difficulty_score", 0.5), # ä¿æŒåŸå€¼
                keywords=question.get("keywords", []), # ä¿æŒåŸå€¼
                cluster_id=question.get("cluster_id"), # ä¿æŒåŸå€¼
                response_draft=draft,    # <--- æ›´æ–°é‡é»
                summary=summary          # <--- æ›´æ–°é‡é»
            )
            
            await question_service.update_ai_analysis(qid, result)
            print(f"âœ… å·²ç‚ºå•é¡Œ {qid} ç”Ÿæˆè‰ç¨¿")
            
        except Exception as e:
            print(f"âŒ è‰ç¨¿ç”Ÿæˆå¤±æ•—: {str(e)}")

    # 3. åŠ å…¥èƒŒæ™¯ä»»å‹™ (è®“ API ç«‹åˆ»å›æ‡‰ï¼Œä¸ç”¨ç­‰ AI)
    background_tasks.add_task(
        _generate_and_save_draft, 
        question_id, 
        question["question_text"]
    )

    return {
        "success": True,
        "message": "å·²é–‹å§‹ç”Ÿæˆè‰ç¨¿ï¼Œè«‹ç¨å¾Œé‡æ–°æ•´ç†é é¢æŸ¥çœ‹"
    }


@router.post("/clusters/generate", summary="åŸ·è¡Œèª²ç¨‹ä¸»é¡Œèšé¡åˆ†æ")
async def generate_course_clusters(
    course_id: str,
    background_tasks: BackgroundTasks
):
    """
    åˆ†æè©²èª²ç¨‹æ‰€æœ‰ã€Œæœªæ­¸é¡ã€çš„å•é¡Œï¼Œå˜—è©¦é€²è¡Œè‡ªå‹•åˆ†ç¾¤èˆ‡å‘½å
    """
    # å®šç¾©èƒŒæ™¯ä»»å‹™
    async def _run_clustering_task(cid: str):
        print(f"ğŸ¤– é–‹å§‹åŸ·è¡Œèª²ç¨‹ {cid} çš„èšé¡åˆ†æ...")
        try:
            # 1. æ’ˆå‡ºè©²èª²ç¨‹æ‰€æœ‰é‚„æ²’åˆ†ç¾¤çš„å•é¡Œ (Pending + Cluster=None)
            questions = await question_service.get_pending_questions_for_ai(cid, limit=50)
            
            if not questions:
                print("æ²’æœ‰éœ€è¦åˆ†ç¾¤çš„å•é¡Œ")
                return

            # ç°¡åŒ–ç‰ˆé‚è¼¯ï¼šç›´æ¥æŠŠå‰ 10 å€‹å•é¡Œä¸Ÿçµ¦ AI è«‹å®ƒæ­¸ç´ä¸€å€‹ä¸»é¡Œ
            # (å¯¦å‹™ä¸Šé€™è£¡å¯ä»¥ç”¨ K-Means æˆ–æ›´è¤‡é›œçš„é‚è¼¯ï¼Œä½†å…ˆå¾ç°¡å–®çš„é–‹å§‹)
            q_texts = [q['question_text'] for q in questions]
            
            # å‘¼å« AI æ­¸ç´ä¸»é¡Œ
            cluster_result = await ai_service.generate_cluster_label(q_texts)
            
            topic_label = cluster_result.get("topic_label", "æœªå‘½åä¸»é¡Œ")
            summary = cluster_result.get("summary", "")
            
            print(f"ğŸ” AI æ­¸ç´å‡ºçš„ä¸»é¡Œ: {topic_label}")
            
            # TODO: é€™è£¡æ‡‰è©²è¦å‘¼å« service æŠŠé€™äº›å•é¡Œçš„ cluster_id æ›´æ–°
            # ä¸¦ä¸”å»ºç«‹ä¸€å€‹æ–°çš„ Cluster Document
            # (é€™éƒ¨åˆ†é‚è¼¯è¼ƒè¤‡é›œï¼Œå»ºè­°å…ˆå¯¦ä½œåˆ°é€™è£¡ç¢ºèª AI èƒ½è·‘)
            
        except Exception as e:
            print(f"âŒ èšé¡åˆ†æå¤±æ•—: {str(e)}")

    background_tasks.add_task(_run_clustering_task, course_id)

    return {
        "success": True,
        "message": "èšé¡åˆ†æä»»å‹™å·²å•Ÿå‹•"
    }

@router.get("/clusters/{course_id}", response_model=dict, summary="å–å¾—èª²ç¨‹çš„æ‰€æœ‰èšé¡")
async def get_clusters_summary(course_id: str):
    """
    å–å¾—èª²ç¨‹çš„æ‰€æœ‰ AI èšé¡æ‘˜è¦
    
    è¿”å›æ¯å€‹èšé¡çš„ï¼š
    - cluster_id
    - æå•æ•¸é‡
    - å¹³å‡é›£åº¦
    - ä»£è¡¨æ€§é—œéµå­—
    """
    from ..database import db
    
    database = db.get_db()
    collection = database["questions"]
    
    pipeline = [
        {
            "$match": {
                "course_id": course_id,
                "cluster_id": {"$ne": None}
            }
        },
        {
            "$group": {
                "_id": "$cluster_id",
                "count": {"$sum": 1},
                "avg_difficulty": {"$avg": "$difficulty_score"},
                "keywords": {"$push": "$keywords"}
            }
        },
        {
            "$sort": {"count": -1}
        }
    ]
    
    results = await collection.aggregate(pipeline).to_list(length=None)
    
    # è™•ç†é—œéµå­—ï¼šå±•å¹³ä¸¦çµ±è¨ˆé »ç‡
    clusters = []
    for result in results:
        all_keywords = []
        for kw_list in result["keywords"]:
            all_keywords.extend(kw_list)
        
        # çµ±è¨ˆé—œéµå­—é »ç‡
        keyword_freq = {}
        for kw in all_keywords:
            keyword_freq[kw] = keyword_freq.get(kw, 0) + 1
        
        # å–å‰ 5 å€‹æœ€å¸¸è¦‹çš„é—œéµå­—
        top_keywords = sorted(
            keyword_freq.items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]
        
        clusters.append({
            "cluster_id": result["_id"],
            "question_count": result["count"],
            "avg_difficulty": result.get("avg_difficulty", 0),
            "top_keywords": [kw[0] for kw in top_keywords]
        })
    
    return {
        "success": True,
        "data": clusters,
        "total_clusters": len(clusters)
    }

