"""
AI å±¤æ•´åˆ API
æä¾› AI/NLP æœå‹™èª¿ç”¨çš„å°ˆç”¨æ¥å£
"""
from fastapi import APIRouter, HTTPException, Query, BackgroundTasks
from typing import List, Optional
from bson import ObjectId
from ..models.schemas import AIAnalysisRequest, AIAnalysisResult, Cluster
from ..services.question_service import question_service
from ..services.ai_service import ai_service


router = APIRouter(prefix="/ai", tags=["ai-integration"])


@router.get("/questions/pending", response_model=dict, summary="å–å¾—å¾… AI åˆ†æçš„æå•")
async def get_pending_questions_for_ai(
    course_id: str = Query(..., description="èª²ç¨‹ID"),
    limit: int = Query(100, ge=1, le=500, description="é™åˆ¶ç­†æ•¸")
):
    """
    å–å¾—å¾… AI åˆ†æçš„æå•åˆ—è¡¨
    
    **æ­¤ API åƒ…è¿”å›å»è­˜åˆ¥åŒ–å¾Œçš„è³‡æ–™**ï¼š
    - pseudonym (å»è­˜åˆ¥åŒ–ä»£è™Ÿ)
    - question_text (æå•å…§å®¹)
    - ä¸åŒ…å«ä»»ä½•å¯è­˜åˆ¥å€‹äººèº«ä»½çš„è³‡è¨Š
    
    **ç”± AI/NLP æœå‹™å®šæœŸèª¿ç”¨**
    """
    questions = await question_service.get_pending_questions_for_ai(
        course_id, limit
    )
    
    return {
        "success": True,
        "data": questions,
        "total": len(questions)
    }


@router.post("/analysis/batch", response_model=dict, summary="æ‰¹æ¬¡å¯«å…¥ AI åˆ†æçµæœ")
async def batch_update_ai_analysis(
    results: List[AIAnalysisResult]
):
    """
    æ‰¹æ¬¡å¯«å…¥ AI åˆ†æçµæœ
    
    **æ­¤ API ç”± AI/NLP æœå‹™èª¿ç”¨**
    
    æ¥æ”¶ AI åˆ†æçµæœä¸¦æ›´æ–°è‡³è³‡æ–™åº«ï¼š
    - cluster_id: AI èšé¡ID
    - difficulty_score: é›£åº¦åˆ†æ•¸ (0-1)
    - keywords: é—œéµå­—åˆ—è¡¨
    """
    success_count = 0
    failed_count = 0
    errors = []
    
    for result in results:
        try:
            question = await question_service.update_ai_analysis(
                result.question_id,
                result
            )
            if question:
                success_count += 1
            else:
                failed_count += 1
                errors.append({
                    "question_id": result.question_id,
                    "error": "æ‰¾ä¸åˆ°æ­¤æå•"
                })
        except Exception as e:
            failed_count += 1
            errors.append({
                "question_id": result.question_id,
                "error": str(e)
            })
    
    return {
        "success": True,
        "message": f"æˆåŠŸæ›´æ–° {success_count} ç­†ï¼Œå¤±æ•— {failed_count} ç­†",
        "success_count": success_count,
        "failed_count": failed_count,
        "errors": errors if errors else None
    }


@router.post("/analysis/single", response_model=dict, summary="å–®ç­†å¯«å…¥ AI åˆ†æçµæœ")
async def single_update_ai_analysis(
    result: AIAnalysisResult
):
    """
    å–®ç­†å¯«å…¥ AI åˆ†æçµæœ
    
    **æ­¤ API ç”± AI/NLP æœå‹™èª¿ç”¨**
    """
    question = await question_service.update_ai_analysis(
        result.question_id,
        result
    )
    
    if not question:
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æ­¤æå•")
    
    return {
        "success": True,
        "message": "AI åˆ†æçµæœæ›´æ–°æˆåŠŸ",
        "data": question
    }

@router.post("/questions/{question_id}/draft", summary="ç”Ÿæˆ/é‡å¯«å•é¡Œçš„å›è¦†è‰ç¨¿")
async def generate_response_draft(
    question_id: str,
    background_tasks: BackgroundTasks
):
    """
    è§¸ç™¼ AI ç‚ºç‰¹å®šå•é¡Œç”Ÿæˆå›è¦†è‰ç¨¿
    """
    # 1. å–å¾—å•é¡Œè³‡æ–™
    question = await question_service.get_question(question_id)
    if not question:
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æ­¤æå•")

    # 2. å®šç¾©èƒŒæ™¯ä»»å‹™å‡½æ•¸
    async def _generate_and_save_draft(qid: str, text: str):
        try:
            # å‘¼å« AI ç”Ÿæˆè‰ç¨¿
            draft = ai_service.generate_response_draft(text)
            
            # å‘¼å« AI ç”Ÿæˆæ‘˜è¦ (é †ä¾¿åš)
            analysis = ai_service.analyze_question(text)
            summary = analysis.get("summary", "")

            new_difficulty = analysis.get("difficulty_score")
            if new_difficulty is None:
                # å˜—è©¦è®€å–èˆŠè³‡æ–™ï¼Œå¦‚æœèˆŠè³‡æ–™ä¹Ÿæ˜¯ Noneï¼Œå°±çµ¦ 0.5
                old_diff = question.get("difficulty_score")
                new_difficulty = old_diff if old_diff is not None else 0.5

            new_keywords = analysis.get("keywords")
            if new_keywords is None:
                 new_keywords = question.get("keywords") or []
            
            # æ§‹é€ æ›´æ–°ç‰©ä»¶ (åˆ©ç”¨ç¾æœ‰çš„ update_ai_analysis ä»‹é¢)
            # æ³¨æ„ï¼šé€™è£¡å‡è¨­æ‚¨å·²ç¶“åœ¨ schemas.py çš„ AIAnalysisResult åŠ å…¥äº† response_draft æ¬„ä½
            result = AIAnalysisResult(
                question_id=qid,
                difficulty_score=float(new_difficulty), # ä¿æŒåŸå€¼
                keywords=new_keywords, # ä¿æŒåŸå€¼
                cluster_id=question.get("cluster_id"), # ä¿æŒåŸå€¼
                response_draft=draft,    # <--- æ›´æ–°é‡é»
                summary=summary          # <--- æ›´æ–°é‡é»
            )
            
            await question_service.update_ai_analysis(qid, result)
            print(f"âœ… å·²ç‚ºå•é¡Œ {qid} ç”Ÿæˆè‰ç¨¿")
            
        except Exception as e:
            print(f"âŒ è‰ç¨¿ç”Ÿæˆå¤±æ•—: {str(e)}")
            import traceback
            traceback.print_exc()

    # 3. åŠ å…¥èƒŒæ™¯ä»»å‹™ (è®“ API ç«‹åˆ»å›æ‡‰ï¼Œä¸ç”¨ç­‰ AI)
    background_tasks.add_task(
        _generate_and_save_draft, 
        question_id, 
        question["question_text"]
    )

    return {
        "success": True,
        "message": "å·²é–‹å§‹ç”Ÿæˆè‰ç¨¿ï¼Œè«‹ç¨å¾Œé‡æ–°æ•´ç†é é¢æŸ¥çœ‹"
    }


@router.post("/clusters/generate", summary="åŸ·è¡Œèª²ç¨‹ä¸»é¡Œèšé¡åˆ†æ")
async def generate_course_clusters(
    course_id: str,
    background_tasks: BackgroundTasks
):
    """
    åˆ†æè©²èª²ç¨‹æ‰€æœ‰ã€Œæœªæ­¸é¡ã€çš„å•é¡Œï¼Œå˜—è©¦é€²è¡Œè‡ªå‹•åˆ†ç¾¤èˆ‡å‘½å
    """
    # å®šç¾©èƒŒæ™¯ä»»å‹™
    async def _run_clustering_task(cid: str):
        print(f"ğŸ¤– [å‡ç´šç‰ˆ] é–‹å§‹åŸ·è¡Œèª²ç¨‹ {cid} çš„å¤šç¶­èšé¡åˆ†æ...")
        from ..database import db
        from bson import ObjectId
        from datetime import datetime
        
        try:
            # 1. æ’ˆå‡ºå¾…è™•ç†å•é¡Œ
            questions = await question_service.get_pending_questions_for_ai(cid, limit=50)
            if not questions:
                print("æ²’æœ‰éœ€è¦åˆ†ç¾¤çš„å•é¡Œ")
                return

            q_texts = [q['question_text'] for q in questions]
            
            # ğŸ”¥ ä¿®æ”¹é»ï¼šæ”¹å‘¼å«æ–°çš„åˆ†ç¾¤æ–¹æ³•
            # (è«‹ç¢ºèª ai_service å·²ç¶“æœ‰ perform_advanced_clustering æ–¹æ³•)
            ai_result = ai_service.perform_advanced_clustering(q_texts)
            
            # é˜²å‘†ï¼šç¢ºä¿å›å‚³çµæ§‹æ­£ç¢º
            if not ai_result or "clusters" not in ai_result:
                print("âŒ AI å›å‚³æ ¼å¼éŒ¯èª¤ï¼Œç„¡æ³•åˆ†ç¾¤")
                return

            clusters_data = ai_result.get("clusters", [])
            print(f"ğŸ“Š AI å°‡å•é¡Œåˆ†æˆäº† {len(clusters_data)} å€‹ç¾¤çµ„")
            
            database = db.get_db()
            
            # 2. éæ­· AI åˆ†å¥½çš„æ¯ä¸€å€‹ç¾¤çµ„
            for cluster_data in clusters_data:
                topic_label = cluster_data.get("topic_label", "æœªå‘½åç¾¤çµ„")
                indices = cluster_data.get("question_indices", []) # é€™æ˜¯ [0, 1, 4...]
                
                if not indices:
                    continue
                    
                print(f"  ğŸ“‚ è™•ç†ç¾¤çµ„: {topic_label} (åŒ…å« {len(indices)} é¡Œ)")
                
                # A. å»ºç«‹ Cluster æ–‡ä»¶
                new_cluster_id = ObjectId()
                new_cluster_doc = {
                    "_id": new_cluster_id,
                    "course_id": cid, # é€™è£¡å‡è¨­ cid æ˜¯ string
                    "topic_label": topic_label,
                    "summary": cluster_data.get("summary", ""),
                    "keywords": [], 
                    "question_count": len(indices),
                    "avg_difficulty": 0.0, 
                    "created_at": datetime.utcnow(),
                    "updated_at": datetime.utcnow()
                }
                await database["clusters"].insert_one(new_cluster_doc)
                
                # B. æ‰¾å‡ºé€™å€‹ç¾¤çµ„å°æ‡‰çš„ Question IDs
                # å› ç‚º AI å›å‚³çš„æ˜¯ index (0, 1, 2...)ï¼Œæˆ‘å€‘è¦æ˜ å°„å› questions é™£åˆ—è£¡çš„ _id
                target_q_ids = []
                for idx in indices:
                    # é˜²å‘†ï¼šç¢ºä¿ index æ²’æœ‰è¶…å‡ºç¯„åœ
                    if isinstance(idx, int) and 0 <= idx < len(questions):
                        target_q_ids.append(ObjectId(questions[idx]['_id']))
                
                # C. æ‰¹æ¬¡æ›´æ–°é€™äº›å•é¡Œçš„ cluster_id
                if target_q_ids:
                    await database["questions"].update_many(
                        {"_id": {"$in": target_q_ids}},
                        {"$set": {
                            "cluster_id": str(new_cluster_id),
                            "updated_at": datetime.utcnow()
                        }}
                    )

            print(f"âœ… å¤šç¶­èšé¡åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ èšé¡åˆ†æå¤±æ•—: {str(e)}")
            import traceback
            traceback.print_exc()

    background_tasks.add_task(_run_clustering_task, course_id)

    return {
        "success": True,
        "message": "èšé¡åˆ†æä»»å‹™å·²å•Ÿå‹•"
    }

@router.get("/clusters/{course_id}", response_model=dict, summary="å–å¾—èª²ç¨‹çš„æ‰€æœ‰èšé¡")
async def get_clusters_summary(course_id: str):
    """
    å–å¾—èª²ç¨‹çš„æ‰€æœ‰ AI èšé¡æ‘˜è¦
    
    è¿”å›æ¯å€‹èšé¡çš„ï¼š
    - cluster_id
    - æå•æ•¸é‡
    - å¹³å‡é›£åº¦
    - ä»£è¡¨æ€§é—œéµå­—
    """
    from ..database import db
    
    database = db.get_db()
    collection = database["questions"]

    # 1. æŸ¥è©¢æ¢ä»¶ï¼šåŒæ™‚æ”¯æ´ String èˆ‡ ObjectId æ ¼å¼çš„ course_id
    match_condition = {
        "$or": [
            {"course_id": course_id},               
            {"course_id": ObjectId(course_id)}      
        ],
        "cluster_id": {"$ne": None}                 
    }
    
    pipeline = [
        {"$match": match_condition},
        {"$group": {
            "_id": "$cluster_id",
            "count": {"$sum": 1},
            # æ³¨æ„ï¼šå¦‚æœè³‡æ–™åº«æ²’æœ‰ difficulty_score æ¬„ä½ï¼Œé€™è£¡æœƒæ˜¯ null
            "avg_difficulty": {"$avg": "$difficulty_score"},
            # ğŸ”¥ ä¿®æ­£ 1ï¼šå¿…é ˆæŠŠé—œéµå­—æ”¶é›†èµ·ä¾†ï¼Œä¸‹é¢çš„è¿´åœˆæ‰è®€å¾—åˆ°
            "keywords": {"$push": "$keywords"} 
        }}
    ]
    
    results = await collection.aggregate(pipeline).to_list(length=None)

    clusters_collection = database["clusters"]
    
    clusters = []
    for result in results:
        # 2. è™•ç†é—œéµå­—ï¼šå¾ questions èšåˆçµæœè¨ˆç®— Top 5
        all_keywords = []
        # åŠ ä¸Šé˜²å‘†ï¼Œç¢ºä¿ keywords å­˜åœ¨ä¸”æ˜¯åˆ—è¡¨
        raw_keywords = result.get("keywords", [])
        for kw_list in raw_keywords:
            if isinstance(kw_list, list):
                all_keywords.extend(kw_list)
        
        # çµ±è¨ˆé »ç‡
        keyword_freq = {}
        for kw in all_keywords:
            if kw: # æ’é™¤ç©ºå­—ä¸²
                keyword_freq[kw] = keyword_freq.get(kw, 0) + 1
        
        # å–å‰ 5 å€‹
        top_keywords = sorted(
            keyword_freq.items(),
            key=lambda x: x[1],
            reverse=True
        )[:5]
        
        # 3. å–å¾— Cluster è©³ç´°è³‡è¨Š (Topic Label)
        topic_label = "æœªå‘½åä¸»é¡Œ"
        try:
            cluster_oid = ObjectId(result["_id"])
            cluster_info = await clusters_collection.find_one({"_id": cluster_oid})
            if cluster_info:
                topic_label = cluster_info.get("topic_label", "æœªå‘½åä¸»é¡Œ")
        except:
            pass # ID æ ¼å¼éŒ¯èª¤æˆ–å…¶ä»–å•é¡Œå‰‡å¿½ç•¥
            
        # ğŸ”¥ ä¿®æ­£ 2ï¼šç¢ºä¿ avg_difficulty çµ•å°ä¸æ˜¯ None
        # å¦‚æœæ˜¯ Noneï¼Œå‰‡å¼·åˆ¶è½‰ç‚º 0ï¼Œé¿å…å‰ç«¯ toFixed å ±éŒ¯
        avg_diff = result.get("avg_difficulty")
        if avg_diff is None:
            avg_diff = 0.0

        clusters.append({
            "cluster_id": str(result["_id"]),
            "topic_label": topic_label,
            "question_count": result["count"],
            "avg_difficulty": avg_diff, # é€™è£¡å‚³å‡ºå»çš„ä¸€å®šæ˜¯æ•¸å­—
            "top_keywords": [kw[0] for kw in top_keywords]
        })
    
    return {
        "success": True,
        "data": clusters,
        "total_clusters": len(clusters)
    }

