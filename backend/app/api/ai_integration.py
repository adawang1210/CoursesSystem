"""
AI å±¤æ•´åˆ API
æä¾› AI/NLP æœå‹™èª¿ç”¨çš„å°ˆç”¨æ¥å£
"""
from fastapi import APIRouter, HTTPException, Query, BackgroundTasks
from typing import List, Optional
from bson import ObjectId
from pydantic import BaseModel  # ğŸ”¥ æ–°å¢é€™è¡Œå¼•å…¥ BaseModel
from ..models.schemas import (
    AIAnalysisRequest, 
    AIAnalysisResult, 
    ClusterGenerateRequest, # ç¢ºä¿ä¹Ÿæœ‰å¼•å…¥é€™å€‹
    ClusterUpdate           # ğŸ”¥ è«‹ç¢ºèªå¼•å…¥çš„æ˜¯é€™å€‹åç¨±
)
from ..services.question_service import question_service
from ..services.ai_service import ai_service


router = APIRouter(prefix="/ai", tags=["ai-integration"])


@router.get("/questions/pending", response_model=dict, summary="å–å¾—å¾… AI åˆ†æçš„æå•")
async def get_pending_questions_for_ai(
    course_id: str = Query(..., description="èª²ç¨‹ID"),
    limit: int = Query(100, ge=1, le=500, description="é™åˆ¶ç­†æ•¸")
):
    """
    å–å¾—å¾… AI åˆ†æçš„æå•åˆ—è¡¨
    
    **æ­¤ API åƒ…è¿”å›å»è­˜åˆ¥åŒ–å¾Œçš„è³‡æ–™**ï¼š
    - pseudonym (å»è­˜åˆ¥åŒ–ä»£è™Ÿ)
    - question_text (æå•å…§å®¹)
    - ä¸åŒ…å«ä»»ä½•å¯è­˜åˆ¥å€‹äººèº«ä»½çš„è³‡è¨Š
    
    **ç”± AI/NLP æœå‹™å®šæœŸèª¿ç”¨**
    """
    questions = await question_service.get_pending_questions_for_ai(
        course_id, limit
    )
    
    return {
        "success": True,
        "data": questions,
        "total": len(questions)
    }


@router.post("/analysis/batch", response_model=dict, summary="æ‰¹æ¬¡å¯«å…¥ AI åˆ†æçµæœ")
async def batch_update_ai_analysis(
    results: List[AIAnalysisResult]
):
    """
    æ‰¹æ¬¡å¯«å…¥ AI åˆ†æçµæœ
    
    **æ­¤ API ç”± AI/NLP æœå‹™èª¿ç”¨**
    
    æ¥æ”¶ AI åˆ†æçµæœä¸¦æ›´æ–°è‡³è³‡æ–™åº«ï¼š
    - cluster_id: AI èšé¡ID
    - difficulty_score: é›£åº¦åˆ†æ•¸ (0-1)
    - keywords: é—œéµå­—åˆ—è¡¨
    """
    success_count = 0
    failed_count = 0
    errors = []
    
    for result in results:
        try:
            question = await question_service.update_ai_analysis(
                result.question_id,
                result
            )
            if question:
                success_count += 1
            else:
                failed_count += 1
                errors.append({
                    "question_id": result.question_id,
                    "error": "æ‰¾ä¸åˆ°æ­¤æå•"
                })
        except Exception as e:
            failed_count += 1
            errors.append({
                "question_id": result.question_id,
                "error": str(e)
            })
    
    return {
        "success": True,
        "message": f"æˆåŠŸæ›´æ–° {success_count} ç­†ï¼Œå¤±æ•— {failed_count} ç­†",
        "success_count": success_count,
        "failed_count": failed_count,
        "errors": errors if errors else None
    }


@router.post("/analysis/single", response_model=dict, summary="å–®ç­†å¯«å…¥ AI åˆ†æçµæœ")
async def single_update_ai_analysis(
    result: AIAnalysisResult
):
    """
    å–®ç­†å¯«å…¥ AI åˆ†æçµæœ
    
    **æ­¤ API ç”± AI/NLP æœå‹™èª¿ç”¨**
    """
    question = await question_service.update_ai_analysis(
        result.question_id,
        result
    )
    
    if not question:
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æ­¤æå•")
    
    return {
        "success": True,
        "message": "AI åˆ†æçµæœæ›´æ–°æˆåŠŸ",
        "data": question
    }

@router.post("/questions/{question_id}/draft", summary="ç”Ÿæˆ/é‡å¯«å•é¡Œçš„å›è¦†è‰ç¨¿")
async def generate_response_draft(
    question_id: str,
    background_tasks: BackgroundTasks
):
    """
    è§¸ç™¼ AI ç‚ºç‰¹å®šå•é¡Œç”Ÿæˆå›è¦†è‰ç¨¿
    """
    # 1. å–å¾—å•é¡Œè³‡æ–™
    question = await question_service.get_question(question_id)
    if not question:
        raise HTTPException(status_code=404, detail="æ‰¾ä¸åˆ°æ­¤æå•")

    # 2. å®šç¾©èƒŒæ™¯ä»»å‹™å‡½æ•¸
    async def _generate_and_save_draft(qid: str, text: str):
        try:
            # å‘¼å« AI ç”Ÿæˆè‰ç¨¿
            draft = ai_service.generate_response_draft(text)
            
            # å‘¼å« AI ç”Ÿæˆæ‘˜è¦ (é †ä¾¿åš)
            analysis = ai_service.analyze_question(text)
            summary = analysis.get("summary", "")

            new_difficulty = analysis.get("difficulty_score")
            if new_difficulty is None:
                # å˜—è©¦è®€å–èˆŠè³‡æ–™ï¼Œå¦‚æœèˆŠè³‡æ–™ä¹Ÿæ˜¯ Noneï¼Œå°±çµ¦ 0.5
                old_diff = question.get("difficulty_score")
                new_difficulty = old_diff if old_diff is not None else 0.5

            new_keywords = analysis.get("keywords")
            if new_keywords is None:
                 new_keywords = question.get("keywords") or []
            
            # æ§‹é€ æ›´æ–°ç‰©ä»¶ (åˆ©ç”¨ç¾æœ‰çš„ update_ai_analysis ä»‹é¢)
            # æ³¨æ„ï¼šé€™è£¡å‡è¨­æ‚¨å·²ç¶“åœ¨ schemas.py çš„ AIAnalysisResult åŠ å…¥äº† response_draft æ¬„ä½
            result = AIAnalysisResult(
                question_id=qid,
                difficulty_score=float(new_difficulty), # ä¿æŒåŸå€¼
                keywords=new_keywords, # ä¿æŒåŸå€¼
                cluster_id=question.get("cluster_id"), # ä¿æŒåŸå€¼
                response_draft=draft,    # <--- æ›´æ–°é‡é»
                summary=summary          # <--- æ›´æ–°é‡é»
            )
            
            await question_service.update_ai_analysis(qid, result)
            print(f"âœ… å·²ç‚ºå•é¡Œ {qid} ç”Ÿæˆè‰ç¨¿")
            
        except Exception as e:
            print(f"âŒ è‰ç¨¿ç”Ÿæˆå¤±æ•—: {str(e)}")
            import traceback
            traceback.print_exc()

    # 3. åŠ å…¥èƒŒæ™¯ä»»å‹™ (è®“ API ç«‹åˆ»å›æ‡‰ï¼Œä¸ç”¨ç­‰ AI)
    background_tasks.add_task(
        _generate_and_save_draft, 
        question_id, 
        question["question_text"]
    )

    return {
        "success": True,
        "message": "å·²é–‹å§‹ç”Ÿæˆè‰ç¨¿ï¼Œè«‹ç¨å¾Œé‡æ–°æ•´ç†é é¢æŸ¥çœ‹"
    }


@router.post("/clusters/generate", summary="åŸ·è¡Œèª²ç¨‹ä¸»é¡Œèšé¡åˆ†æ")
async def generate_course_clusters(
    request: ClusterGenerateRequest,  # ğŸ”¥ ä¿®æ”¹ 1ï¼šæ”¹ç”¨ Pydantic Model æ¥æ”¶ JSON Body
    background_tasks: BackgroundTasks
):
    """
    åˆ†æè©²èª²ç¨‹æ‰€æœ‰ã€Œæœªæ­¸é¡ã€çš„å•é¡Œï¼Œå˜—è©¦é€²è¡Œè‡ªå‹•åˆ†ç¾¤èˆ‡å‘½å
    """
    course_id = request.course_id
    max_clusters = request.max_clusters

    # å®šç¾©èƒŒæ™¯ä»»å‹™
    async def _run_clustering_task(cid: str, max_c: int):
        print(f"ğŸ¤– [æ™ºèƒ½æ­¸æª”æ¨¡å¼] é–‹å§‹åˆ†æèª²ç¨‹ {cid} (ç¸½ä¸Šé™ {max_c} çµ„)...")
        from ..database import db
        from bson import ObjectId
        from datetime import datetime
        
        try:
            database = db.get_db()

            # -------------------------------------------------------
            # ğŸ”¥ æ­¥é©Ÿ 1: å…ˆæ’ˆå‡ºã€Œç¾æœ‰çš„ã€èšé¡æ¨™ç±¤ (Context)
            # -------------------------------------------------------
            existing_clusters_cursor = database["clusters"].find({"course_id": cid})
            existing_clusters = await existing_clusters_cursor.to_list(length=None)
            
            # å»ºç«‹æŸ¥è¡¨å­—å…¸
            existing_topic_map = {c["topic_label"]: c["_id"] for c in existing_clusters}
            existing_topic_names = list(existing_topic_map.keys())
            
            # ğŸ”¥ é—œéµä¿®æ”¹ï¼šè¨ˆç®—ã€Œå‰©é¤˜é¡åº¦ã€
            current_count = len(existing_topic_names)
            remaining_quota = max_c - current_count
            
            # è‹¥æ—¢æœ‰åˆ†é¡å·²è¶…éæˆ–ç­‰æ–¼ä¸Šé™ï¼Œå‰‡ä¸å…è¨±æ–°å¢ (æˆ–è¨­ç‚º 0)
            if remaining_quota < 0:
                remaining_quota = 0
                
            print(f"ğŸ“š ç‹€æ…‹: æ—¢æœ‰ {current_count} çµ„ | ä¸Šé™ {max_c} çµ„ | ğŸ’¡ å¯æ–°å¢ {remaining_quota} çµ„")

            # -------------------------------------------------------
            # ğŸ”¥ æ­¥é©Ÿ 2: æ’ˆå‡ºã€Œæœªåˆ†é¡ã€çš„å•é¡Œ
            # -------------------------------------------------------
            questions = await question_service.get_pending_questions_for_ai(cid, limit=50)
            
            if not questions:
                print("âœ… æ²’æœ‰æ–°çš„æœªåˆ†é¡å•é¡Œï¼Œå·¥ä½œçµæŸ")
                return

            q_texts = [q['question_text'] for q in questions]
            
            # -------------------------------------------------------
            # ğŸ”¥ æ­¥é©Ÿ 3: å‘¼å« AI (å‚³å…¥è¨ˆç®—å¾Œçš„é¡åº¦)
            # -------------------------------------------------------
            # æ³¨æ„ï¼šé€™è£¡çš„åƒæ•¸åç¨±å¿…é ˆèˆ‡ ai_service.py å®šç¾©çš„ä¸€è‡´ (max_new_topics)
            ai_result = ai_service.perform_advanced_clustering(
                q_texts, 
                max_new_topics=remaining_quota,  # <--- æ”¹ç”¨é€™å€‹åƒæ•¸
                existing_topics=existing_topic_names
            )
            
            if not ai_result or "clusters" not in ai_result:
                print("âŒ AI å›å‚³æ ¼å¼éŒ¯èª¤")
                return

            clusters_data = ai_result.get("clusters", [])
            print(f"ğŸ“Š AI å°‡ {len(q_texts)} å€‹æ–°å•é¡Œåˆ†æˆäº† {len(clusters_data)} çµ„")

            # -------------------------------------------------------
            # ğŸ”¥ æ­¥é©Ÿ 4: æ™ºæ…§å¯«å…¥ (æ¯”å°æ–°èˆŠ)
            # -------------------------------------------------------
            # (é€™éƒ¨åˆ†çš„é‚è¼¯ä¿æŒä¸è®Šï¼Œè² è²¬å°‡ AI çµæœå¯«å…¥è³‡æ–™åº«)
            for cluster_data in clusters_data:
                topic_label = cluster_data.get("topic_label", "æœªå‘½åç¾¤çµ„")
                indices = cluster_data.get("question_indices", [])
                
                if not indices:
                    continue
                
                if topic_label in existing_topic_map:
                    # æ­¸å…¥æ—¢æœ‰åˆ†é¡
                    target_cluster_id = existing_topic_map[topic_label]
                    print(f"  ğŸ”„ æ­¸å…¥æ—¢æœ‰åˆ†é¡: {topic_label}")
                    
                    await database["clusters"].update_one(
                        {"_id": target_cluster_id},
                        {
                            "$inc": {"question_count": len(indices)},
                            "$set": {"updated_at": datetime.utcnow()}
                        }
                    )
                else:
                    # å»ºç«‹æ–°åˆ†é¡ (åªæœ‰åœ¨é¡åº¦å…§ AI æ‰æœƒå›å‚³æ–°çš„)
                    print(f"  âœ¨ å»ºç«‹å…¨æ–°åˆ†é¡: {topic_label}")
                    new_cluster_id = ObjectId()
                    target_cluster_id = new_cluster_id
                    
                    new_cluster_doc = {
                        "_id": new_cluster_id,
                        "course_id": cid, 
                        "topic_label": topic_label,
                        "summary": cluster_data.get("summary", ""),
                        "keywords": [], 
                        "question_count": len(indices),
                        "avg_difficulty": 0.0, 
                        "is_locked": False, 
                        "created_at": datetime.utcnow(),
                        "updated_at": datetime.utcnow()
                    }
                    await database["clusters"].insert_one(new_cluster_doc)
                    existing_topic_map[topic_label] = new_cluster_id

                # æ›´æ–°å•é¡Œé—œè¯
                target_q_ids = []
                for idx in indices:
                    if isinstance(idx, int) and 0 <= idx < len(questions):
                        target_q_ids.append(ObjectId(questions[idx]['_id']))
                
                if target_q_ids:
                    await database["questions"].update_many(
                        {"_id": {"$in": target_q_ids}},
                        {"$set": {
                            "cluster_id": str(target_cluster_id),
                            "updated_at": datetime.utcnow()
                        }}
                    )

            print(f"âœ… æ™ºèƒ½åˆ†æå®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ èšé¡åˆ†æå¤±æ•—: {str(e)}")
            import traceback
            traceback.print_exc()

    background_tasks.add_task(_run_clustering_task, course_id, max_clusters)

    return {
        "success": True,
        "message": f"èšé¡åˆ†æä»»å‹™å·²å•Ÿå‹• (åˆ†é¡ä¸Šé™: {max_clusters})"
    }

@router.get("/clusters/{course_id}", response_model=dict, summary="å–å¾—èª²ç¨‹çš„æ‰€æœ‰èšé¡")
async def get_clusters_summary(course_id: str):
    """
    å–å¾—èª²ç¨‹çš„æ‰€æœ‰ AI èšé¡æ‘˜è¦ (åŒ…å«å°šæœªæœ‰æå•çš„ç©ºåˆ†é¡)
    """
    from ..database import db
    
    database = db.get_db()

    # 1. æŸ¥è©¢æ¢ä»¶
    match_condition = {
        "$or": [
            {"course_id": course_id},               
            {"course_id": ObjectId(course_id)}      
        ]
    }
    
    # ğŸ”¥ é—œéµä¿®æ­£ 1ï¼šå…ˆå¾ clusters è¡¨æ’ˆå‡ºæ‰€æœ‰åˆ†é¡çš„ã€Œåº•ç‰ˆã€ (é€™æ¨£ç©ºåˆ†é¡æ‰æœƒå‡ºç¾)
    all_clusters_cursor = database["clusters"].find(match_condition)
    all_clusters = await all_clusters_cursor.to_list(length=None)
    
    # 2. ä¾ç„¶å» questions è¡¨åšèšåˆï¼Œç”¨ä¾†ç²¾æº–è¨ˆç®—ã€Œå„åˆ†é¡æœ‰å¹¾é¡Œã€è·Ÿã€Œé›£åº¦ã€
    q_match = match_condition.copy()
    q_match["cluster_id"] = {"$ne": None}
    
    pipeline = [
        {"$match": q_match},
        {"$group": {
            "_id": "$cluster_id",
            "count": {"$sum": 1},
            "avg_difficulty": {"$avg": "$difficulty_score"},
            "keywords": {"$push": "$keywords"} 
        }}
    ]
    q_stats = await database["questions"].aggregate(pipeline).to_list(length=None)
    
    # å°‡èšåˆçµæœè½‰æˆå­—å…¸æ–¹ä¾¿æŸ¥è¡¨: { "cluster_id_å­—ä¸²": çµ±è¨ˆè³‡æ–™ }
    stats_map = {str(stat["_id"]): stat for stat in q_stats}

    # 3. æŠŠè³‡æ–™çµ„åˆèµ·ä¾†å›å‚³çµ¦å‰ç«¯
    response_data = []
    for cluster in all_clusters:
        c_id_str = str(cluster["_id"])
        stat = stats_map.get(c_id_str)
        
        if stat:
            # å¦‚æœé€™å€‹åˆ†é¡æœ‰æå•ï¼Œå°±å‹•æ…‹è¨ˆç®— Top 5 é—œéµå­—
            all_keywords = []
            for kw_list in stat.get("keywords", []):
                if isinstance(kw_list, list):
                    all_keywords.extend(kw_list)
            
            keyword_freq = {}
            for kw in all_keywords:
                if kw: keyword_freq[kw] = keyword_freq.get(kw, 0) + 1
            
            top_keywords = [kw[0] for kw in sorted(keyword_freq.items(), key=lambda x: x[1], reverse=True)[:5]]
            
            response_data.append({
                "cluster_id": c_id_str,
                "topic_label": cluster.get("topic_label", "æœªå‘½åä¸»é¡Œ"),
                "question_count": stat["count"],
                "avg_difficulty": stat.get("avg_difficulty") or 0.0,
                "top_keywords": top_keywords
            })
        else:
            # ğŸ”¥ é—œéµä¿®æ­£ 2ï¼šå¦‚æœé€™å€‹åˆ†é¡ç›®å‰ã€Œæ²’æœ‰æå•ã€(ä¾‹å¦‚æ‰‹å‹•å‰›æ–°å¢çš„ç©ºåˆ†é¡)
            response_data.append({
                "cluster_id": c_id_str,
                "topic_label": cluster.get("topic_label", "æœªå‘½åä¸»é¡Œ"),
                "question_count": 0,
                "avg_difficulty": 0.0,
                "top_keywords": cluster.get("keywords", [])
            })
            
    return {
        "success": True,
        "data": response_data,
        "total_clusters": len(response_data)
    }
# ç¤ºæ„ï¼šæ–°å¢æ›´æ–° Cluster çš„ API
@router.patch("/clusters/{cluster_id}")
async def update_cluster(cluster_id: str, update_data: ClusterUpdate):
    """
    [æ–°å¢] æ‰‹å‹•æ›´æ–°èšé¡æ¨™ç±¤ (åŠ©æ•™ä»‹å…¥)
    """
    from ..database import db
    from bson import ObjectId
    from datetime import datetime
    
    database = db.get_db()
    
    # 1. æº–å‚™æ›´æ–°æ¬„ä½
    update_fields = {
        "updated_at": datetime.utcnow()
    }
    
    # å¦‚æœæœ‰å‚³å…¥æ–°çš„æ¨™é¡Œï¼Œå°±æ›´æ–° topic_label
    if update_data.topic_label:
        update_fields["topic_label"] = update_data.topic_label
        # åŒæ™‚è¨˜éŒ„é€™æ˜¯äººå·¥è¨­å®šçš„æ¨™ç±¤
        update_fields["manual_label"] = update_data.topic_label

    # å¦‚æœæœ‰æŒ‡å®šé–å®šç‹€æ…‹ (é è¨­ç‚º True)
    if update_data.is_locked is not None:
        update_fields["is_locked"] = update_data.is_locked
        
    # 2. åŸ·è¡Œæ›´æ–°
    result = await database["clusters"].update_one(
        {"_id": ObjectId(cluster_id)},
        {"$set": update_fields}
    )
    
    if result.matched_count == 0:
        return {"success": False, "message": "æ‰¾ä¸åˆ°è©²èšé¡ä¸»é¡Œ"}
        
    return {"success": True, "message": "æ›´æ–°æˆåŠŸ"}


# ğŸ”¥ æ–°å¢ï¼šäººå·¥æ‰‹å‹•å»ºç«‹èšé¡çš„æ¨¡å‹èˆ‡ API
class ManualClusterCreate(BaseModel):
    course_id: str
    topic_label: str

@router.post("/clusters/manual", summary="äººå·¥æ‰‹å‹•æ–°å¢èšé¡ä¸»é¡Œ")
async def create_manual_cluster(request: ManualClusterCreate):
    """
    å…è¨±æ•™å¸«/åŠ©æ•™æ‰‹å‹•å»ºç«‹å…¨æ–°çš„åˆ†é¡ï¼Œå°‡è‡ªå‹•è¦–ç‚ºé–å®šç‹€æ…‹
    """
    from ..database import db
    from bson import ObjectId
    from datetime import datetime
    
    database = db.get_db()
    
    # æª¢æŸ¥æ˜¯å¦å·²æœ‰åŒåæ¨™ç±¤
    existing = await database["clusters"].find_one({
        "course_id": request.course_id, 
        "topic_label": request.topic_label
    })
    
    if existing:
        return {"success": False, "message": "è©²ä¸»é¡Œæ¨™ç±¤å·²å­˜åœ¨"}

    new_cluster = {
        "_id": ObjectId(),
        "course_id": request.course_id,
        "topic_label": request.topic_label,
        "summary": "äººå·¥æ‰‹å‹•å»ºç«‹çš„ä¸»é¡Œ",
        "keywords": [],
        "question_count": 0,
        "avg_difficulty": 0.0,
        "is_locked": True,          # ğŸ”¥ äººå·¥å»ºç«‹çš„é è¨­é–å®šï¼ŒAI ä¸èƒ½äº‚æ”¹
        "manual_label": request.topic_label,
        "created_at": datetime.utcnow(),
        "updated_at": datetime.utcnow()
    }
    
    await database["clusters"].insert_one(new_cluster)
    return {"success": True, "message": "å»ºç«‹æˆåŠŸ"}

@router.delete("/clusters/{cluster_id}", summary="åˆªé™¤èšé¡ä¸»é¡Œ")
async def delete_cluster(cluster_id: str):
    """
    [æ–°å¢] åˆªé™¤åˆ†é¡ã€‚
    åˆªé™¤å¾Œï¼ŒåŸå±¬æ–¼æ­¤åˆ†é¡çš„æå•å°‡æ¢å¾©ç‚ºã€Œæœªåˆ†é¡ã€ç‹€æ…‹ã€‚
    """
    from ..database import db
    from bson import ObjectId
    
    database = db.get_db()
    
    try:
        oid = ObjectId(cluster_id)
    except:
        return {"success": False, "message": "ç„¡æ•ˆçš„åˆ†é¡ ID"}

    # 1. æŠŠé€™å€‹åˆ†é¡è£¡é¢çš„å•é¡Œã€Œé‡‹æ”¾ã€å‡ºä¾† (æŠŠ cluster_id è¨­å› None)
    await database["questions"].update_many(
        {"cluster_id": cluster_id},  # å°‹æ‰¾å±¬æ–¼é€™å€‹åˆ†é¡çš„å•é¡Œ
        {"$set": {"cluster_id": None}} # å°‡å®ƒå€‘è¨­ç‚ºæœªåˆ†é¡
    )
    
    # 2. åˆªé™¤é€™å€‹åˆ†é¡æœ¬èº«
    result = await database["clusters"].delete_one({"_id": oid})
    
    if result.deleted_count == 0:
        return {"success": False, "message": "æ‰¾ä¸åˆ°è©²åˆ†é¡"}
        
    return {"success": True, "message": "åˆ†é¡å·²åˆªé™¤ï¼Œå…§éƒ¨æå•å·²æ¢å¾©æœªåˆ†é¡ç‹€æ…‹"}